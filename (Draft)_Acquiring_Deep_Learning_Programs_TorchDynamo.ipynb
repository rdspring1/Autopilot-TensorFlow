{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdspring1/Autopilot-TensorFlow/blob/master/(Draft)_Acquiring_Deep_Learning_Programs_TorchDynamo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TorchDynamo\n",
        "\n",
        "TorchDynamo uses CPython's frame evaluation API (from [PEP 523](https://peps.python.org/pep-0523/)) to trace the execution of a Python program. This is distinct from TorchScript Scripting, which reads the Python program using the Python \"Abstract Syntax Tree (AST)\", and from TorchScript Tracing, which records PyTorch operations as they're performed. \n",
        "\n",
        "In this notebook we'll look at some simple examples to begin understanding the capabilities and limitations of TorchDynamo. For more information on TorchDynamo, see there posts on PyTorch Dev Discussions, like [this one](https://dev-discuss.pytorch.org/t/torchdynamo-update-8-torchdynamo-passed-correctness-check-on-7k-github-models/663)."
      ],
      "metadata": {
        "id": "tErXl6BAypw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Started with TorchDynamo in Colab\n",
        "\n",
        "TorchDynamo is still experimental, and it's designed to work with the nightly version of PyTorch, so we'll start by configuring our Colab environment. This should take a few minutes, and it will build TorchDynamo from source."
      ],
      "metadata": {
        "id": "vZSvE2mlzy2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstalls Colab's default PyTorch and install PyTorch nightly\n",
        "!pip3 uninstall --yes torch\n",
        "!pip3 install --pre torch --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Gt6ozmPV6PS",
        "outputId": "60e7bb09-753c-4122-9995-3372c723534f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 1.12.1+cu113\n",
            "Uninstalling torch-1.12.1+cu113:\n",
            "  Successfully uninstalled torch-1.12.1+cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/nightly/cpu\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.13.0.dev20221006%2Bcpu-cp37-cp37m-linux_x86_64.whl (198.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 198.6 MB 56 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.13.0.dev20221006+cpu which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.13.0.dev20221006+cpu which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.13.0.dev20221006+cpu which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.13.0.dev20221006+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifies we have the right version\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "8-u5yBXf0Nnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clones the TorchDynamo rep from source\n",
        "!git clone https://github.com/pytorch/torchdynamo.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoE_OJd80Qj2",
        "outputId": "2301a44a-1fc0-4620-8fca-fd93721781b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torchdynamo'...\n",
            "remote: Enumerating objects: 16091, done.\u001b[K\n",
            "remote: Counting objects: 100% (2944/2944), done.\u001b[K\n",
            "remote: Compressing objects: 100% (401/401), done.\u001b[K\n",
            "remote: Total 16091 (delta 2695), reused 2706 (delta 2536), pack-reused 13147\u001b[K\n",
            "Receiving objects: 100% (16091/16091), 6.25 MiB | 29.88 MiB/s, done.\n",
            "Resolving deltas: 100% (12510/12510), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd torchdynamo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NzBgixL0Rfs",
        "outputId": "657a2c6e-eee3-4a7a-f43f-1565aba4a035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/torchdynamo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lTCut4G137V",
        "outputId": "ee3eab20-ff38-49f2-d0be-483885e2ace4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting black==22.8.0\n",
            "  Downloading black-22.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 27.5 MB/s \n",
            "\u001b[?25hCollecting flake8==5.0.4\n",
            "  Downloading flake8-5.0.4-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 499 kB/s \n",
            "\u001b[?25hCollecting isort==5.10.1\n",
            "  Downloading isort-5.10.1-py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 79.0 MB/s \n",
            "\u001b[?25hCollecting mypy==0.960\n",
            "  Downloading mypy-0.960-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 66.6 MB/s \n",
            "\u001b[?25hCollecting click>=8.1\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting expecttest\n",
            "  Downloading expecttest-0.1.3-py3-none-any.whl (6.5 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (2.6.3)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2.4-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 63.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.21.6)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (3.6.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (6.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.7.1)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (1.13.0.dev20221006+cpu)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from black==22.8.0->-r requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from black==22.8.0->-r requirements.txt (line 1)) (4.1.1)\n",
            "Collecting pathspec>=0.9.0\n",
            "  Downloading pathspec-0.10.1-py3-none-any.whl (27 kB)\n",
            "Collecting typed-ast>=1.4.2\n",
            "  Downloading typed_ast-1.5.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 63.7 MB/s \n",
            "\u001b[?25hCollecting platformdirs>=2\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Collecting importlib-metadata<4.3,>=1.1.0\n",
            "  Downloading importlib_metadata-4.2.0-py3-none-any.whl (16 kB)\n",
            "Collecting pyflakes<2.6.0,>=2.5.0\n",
            "  Downloading pyflakes-2.5.0-py2.py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting mccabe<0.8.0,>=0.7.0\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting pycodestyle<2.10.0,>=2.9.0\n",
            "  Downloading pycodestyle-2.9.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 529 kB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.3,>=1.1.0->flake8==5.0.4->-r requirements.txt (line 2)) (3.8.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 10)) (1.4.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 10)) (8.14.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 10)) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 10)) (22.1.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 10)) (1.11.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 10)) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 10)) (57.4.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->-r requirements.txt (line 12)) (1.2.1)\n",
            "Installing collected packages: importlib-metadata, typed-ast, pyflakes, pycodestyle, platformdirs, pathspec, mypy-extensions, mccabe, click, ninja, mypy, isort, flake8, expecttest, black\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 5.0.0\n",
            "    Uninstalling importlib-metadata-5.0.0:\n",
            "      Successfully uninstalled importlib-metadata-5.0.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.4.1 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible.\n",
            "gym 0.25.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\u001b[0m\n",
            "Successfully installed black-22.8.0 click-8.1.3 expecttest-0.1.3 flake8-5.0.4 importlib-metadata-4.2.0 isort-5.10.1 mccabe-0.7.0 mypy-0.960 mypy-extensions-0.4.3 ninja-1.10.2.4 pathspec-0.10.1 platformdirs-2.5.2 pycodestyle-2.9.1 pyflakes-2.5.0 typed-ast-1.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py develop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-BDqXbz19Lv",
        "outputId": "da6a8970-16fc-4647-cd3a-0b82817981f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "running develop\n",
            "running egg_info\n",
            "creating torchdynamo.egg-info\n",
            "writing torchdynamo.egg-info/PKG-INFO\n",
            "writing dependency_links to torchdynamo.egg-info/dependency_links.txt\n",
            "writing requirements to torchdynamo.egg-info/requires.txt\n",
            "writing top-level names to torchdynamo.egg-info/top_level.txt\n",
            "writing manifest file 'torchdynamo.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'torchdynamo.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "building 'torchdynamo._eval_frame' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/torchdynamo\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -c torchdynamo/_eval_frame.c -o build/temp.linux-x86_64-3.7/torchdynamo/_eval_frame.o -Wall\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/torchdynamo\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/torchdynamo/_eval_frame.o -o build/lib.linux-x86_64-3.7/torchdynamo/_eval_frame.cpython-37m-x86_64-linux-gnu.so\n",
            "building 'torchdynamo._guards' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c torchdynamo/_guards.cpp -o build/temp.linux-x86_64-3.7/torchdynamo/_guards.o -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/torchdynamo/_guards.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/torchdynamo/_guards.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.7/torchdynamo/_eval_frame.cpython-37m-x86_64-linux-gnu.so -> torchdynamo\n",
            "copying build/lib.linux-x86_64-3.7/torchdynamo/_guards.cpython-37m-x86_64-linux-gnu.so -> torchdynamo\n",
            "Creating /usr/local/lib/python3.7/dist-packages/torchdynamo.egg-link (link to .)\n",
            "Adding torchdynamo 1.13.0.dev0 to easy-install.pth file\n",
            "\n",
            "Installed /content/torchdynamo\n",
            "Processing dependencies for torchdynamo==1.13.0.dev0\n",
            "Searching for sympy==1.7.1\n",
            "Best match: sympy 1.7.1\n",
            "Adding sympy 1.7.1 to easy-install.pth file\n",
            "Installing isympy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for networkx==2.6.3\n",
            "Best match: networkx 2.6.3\n",
            "Adding networkx 2.6.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Jinja2==2.11.3\n",
            "Best match: Jinja2 2.11.3\n",
            "Adding Jinja2 2.11.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for dill==0.3.5.1\n",
            "Best match: dill 0.3.5.1\n",
            "Adding dill 0.3.5.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for PyYAML==6.0\n",
            "Best match: PyYAML 6.0\n",
            "Adding PyYAML 6.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tabulate==0.8.10\n",
            "Best match: tabulate 0.8.10\n",
            "Adding tabulate 0.8.10 to easy-install.pth file\n",
            "Installing tabulate script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.21.6\n",
            "Best match: numpy 1.21.6\n",
            "Adding numpy 1.21.6 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for torch==1.13.0.dev20221006+cpu\n",
            "Best match: torch 1.13.0.dev20221006+cpu\n",
            "Adding torch 1.13.0.dev20221006+cpu to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for mpmath==1.2.1\n",
            "Best match: mpmath 1.2.1\n",
            "Adding mpmath 1.2.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for MarkupSafe==2.0.1\n",
            "Best match: MarkupSafe 2.0.1\n",
            "Adding MarkupSafe 2.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for typing-extensions==4.1.1\n",
            "Best match: typing-extensions 4.1.1\n",
            "Adding typing-extensions 4.1.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for torchdynamo==1.13.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchdynamo"
      ],
      "metadata": {
        "id": "1y7vR3902Rr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introductory Example\n",
        "We'll start with a very simple example to see how to invoke TorchDynamo and what it produces."
      ],
      "metadata": {
        "id": "rP9nKQsq2klq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "# Clears any previously registered optimizer\n",
        "# NOTE: this is useful if you want to experiment with tweaking the \n",
        "#   dynamo_tabular_printer function below\n",
        "torchdynamo.reset()\n",
        "\n",
        "# A callback to review the FX graphs that TorchDynamo generates\n",
        "def dynamo_printer(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
        "    # gm.graph.print_tabular()\n",
        "    print(gm.code)\n",
        "    return gm.forward"
      ],
      "metadata": {
        "id": "1oRqgoRy2gJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torchdynamo.optimize(dynamo_printer)\n",
        "def foo_simple(a, b):\n",
        "  return a + b\n",
        "\n",
        "a = torch.ones(4)\n",
        "b = torch.arange(4)\n",
        "\n",
        "result = foo_simple(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbCgLwph5lUo",
        "outputId": "d5f1f2cc-3357-4bd8-d544-d0c8a0010dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
            "    add = a + b;  a = b = None\n",
            "    return (add,)\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a lot going on here, so let's break it down.\n",
        "\n",
        "Let's start by considering the `foo_simple` function, which just adds two tensors. In the cell above those two tensors are `a` and `b`. `foo_simple` is decorated with `@torchdynamo.optimize`, and when `foo_simple` is called the Python equivalent of the traced operations is printed.\n",
        "\n",
        "TorchDynamo observes Python frames to create one or more FX `GraphModules`. The function passed to `@torchdynamo.optimize` is then given these `GraphModules` (one at a time, along with the inputs used to generate them). It can do whatever it likes to each graph, and it must return a callable that TorchDynamo will execute instead of the original operation. The intention being that the callable returned might perform the same computation faster than the origial.\n",
        "\n",
        "The `dynamo_printer` function doesn't actually do any optimization, however. It just prints the `GraphModules`'s Python and returns its forward function without modification. FX's definition of `forward` is a little different than our `foo_simple`, but it clearly captures the addition of `a` and `b`.\n",
        "\n",
        "To learn more about FX, see the [FX documentation](https://pytorch.org/docs/stable/fx.html). FX's `GraphModules` are easy to read and transform, and it's convenient that TorchDynamo produces them."
      ],
      "metadata": {
        "id": "YKhpkxId24TJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TorchDynamo Traces Python\n",
        "\n",
        "TorchDynamo, like TorchScript Tracing, is a tracer. It watches Python frames go by, so, like other tracers, it produces \"traces.\" These \"traces\" are sequences of operations without control flow, and they represent one \"path\" through a function. Some functions, like `foo_simple` above, only have one path through them, and so TorchDynamo observes the entire program as it's run. We can see that TorchDynamo is tracing by looking at how it handles control flow, like if/else statements and loops."
      ],
      "metadata": {
        "id": "tVg4FZrP8Ek6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torchdynamo.optimize(dynamo_printer)\n",
        "def foo_branching(a, b):\n",
        "  if a.dtype is torch.float32:\n",
        "    return a + b\n",
        "  return a - b\n",
        "\n",
        "result = foo_branching(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4p-KxOE9AJ7",
        "outputId": "de31b49e-323b-46e9-b7d2-7024c35beac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
            "    add = a + b;  a = b = None\n",
            "    return (add,)\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When `a` is a float32 tensor it's added with `b`, and the trace that TorchDynamo creates from our samples `a` and `b` only shows that addition. Passing `a` as a float64 tensor reveals the other path."
      ],
      "metadata": {
        "id": "OBty0j0n9K_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torchdynamo.optimize(dynamo_printer)\n",
        "def foo_branching(a, b):\n",
        "  if a.dtype is torch.float32:\n",
        "    return a + b\n",
        "  return a - b\n",
        "\n",
        "result = foo_branching(a.double(), b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKxGkQrO9YqI",
        "outputId": "98561916-bbb7-42d1-fc7b-58ff4a12b6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
            "    sub = a - b;  a = b = None\n",
            "    return (sub,)\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since TorchDynamo is tracing it also \"unrolls\" loops. "
      ],
      "metadata": {
        "id": "AHrO2tWQ9eyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torchdynamo.optimize(dynamo_printer)\n",
        "def foo_loop(a):\n",
        "  b = a\n",
        "  for _ in range(3):\n",
        "    b = b + a\n",
        "\n",
        "  return b\n",
        "\n",
        "result = foo_loop(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_s3kvdo9lvq",
        "outputId": "16e5f10e-eac5-4712-a97a-6de41dc89654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "def forward(self, a : torch.Tensor):\n",
            "    add = a + a\n",
            "    add_1 = add + a;  add = None\n",
            "    add_2 = add_1 + a;  add_1 = a = None\n",
            "    return (add_2,)\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TorchDynamo is actually capable of observing the loop since it's looking at Python frames, but the FX `GraphModules` it produces can only represent traces, which don't include control flow. There's a representational trade-off with this approach, as traces may be easier to transform and execute than graphs, which may contain control flow."
      ],
      "metadata": {
        "id": "iuL21I-P9z-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Foreign Functions and Multiple Graphs\n",
        "\n",
        "So far we've seen TorchDynamo capture a single path through a function into a single FX `GraphModule`, but this is not always possible. Functions may include \"foreign\" functions that aren't PyTorch operations, and we don't want these appearing in the FX `GraphModule`. TorchDynamo deals with foreign functions by separating them from the `GraphModules` describing a function. Let's look at some examples."
      ],
      "metadata": {
        "id": "-EHMTGI_8KB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torchdynamo.optimize(dynamo_printer)\n",
        "def foo_foreign(a, b):\n",
        "  a = a + 2\n",
        "  b = torch.from_numpy(b)\n",
        "  return a + b\n",
        "\n",
        "# Suppresses some SymPy warnings not relevant to what we're doing\n",
        "result = foo_foreign(a, b.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPRMQ5hx-u-n",
        "outputId": "ceef80db-1a11-4d84-8e65-f11da9839a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torchdynamo.symbolic_convert: [WARNING] Graph break: call_function args: NumpyVariable()  from user code at   File \"<ipython-input-66-bf3e0f894115>\", line 4, in foo_foreign\n",
            "    b = torch.from_numpy(b)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, a : torch.Tensor):\n",
            "    add = a + 2;  a = None\n",
            "    return (add,)\n",
            "    \n",
            "\n",
            "\n",
            "\n",
            "def forward(self, _stack0 : torch.Tensor, a : torch.Tensor):\n",
            "    add = a + _stack0;  a = _stack0 = None\n",
            "    return (add,)\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`foo_foreign` expects a PyTorch tensor and a NumPy array. When traced, TorchDynamo warns us that our use of `torch.from_numpy` is causing a \"graph break,\" and instead of one trace we get two. The first performs the `a + 2` addition, and the second performs the `a + b` addition. The middle of the function appears to be missing. We can use `torchdynamo.explain` for more information."
      ],
      "metadata": {
        "id": "16jGX1LwAc8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explanation, out_guards, graphs, ops_per_graph, break_reasons = torchdynamo.explain(foo_foreign, a, b.numpy())"
      ],
      "metadata": {
        "id": "AzE8ioP_A74a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(explanation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVdfB17ZBCxD",
        "outputId": "aeb0088a-7bc6-4618-85c8-2b04dc420fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamo produced 2 graphswith 1 graph break and 2 ops\n",
            " Break reasons: \n",
            "\n",
            "1. call_function args: NumpyVariable() \n",
            "  File \"<ipython-input-66-bf3e0f894115>\", line 4, in foo_foreign\n",
            "    b = torch.from_numpy(b)\n",
            " \n",
            "TorchDynamo compilation metrics:\n",
            "Function                                             Runtimes (s)\n",
            "---------------------------------------------------  --------------\n",
            "convert_frame_assert.<locals>._convert_frame_assert  0.0063, 0.0041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like TorchDynamo is unhappy with about our using a NumPy array. Even though the function's middle appears to be missing, running it with TorchDynamo produces the expected result:"
      ],
      "metadata": {
        "id": "tWokdfeABMd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "foo_foreign(a, b.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftQLdq6KBW8u",
        "outputId": "ed8d4af4-0b6b-4a58-ff0f-59374ee7430b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torchdynamo.symbolic_convert: [WARNING] Graph break: call_function args: NumpyVariable()  from user code at   File \"<ipython-input-66-bf3e0f894115>\", line 4, in foo_foreign\n",
            "    b = torch.from_numpy(b)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, a : torch.Tensor):\n",
            "    add = a + 2;  a = None\n",
            "    return (add,)\n",
            "    \n",
            "\n",
            "\n",
            "\n",
            "def forward(self, _stack0 : torch.Tensor, a : torch.Tensor):\n",
            "    add = a + _stack0;  a = _stack0 = None\n",
            "    return (add,)\n",
            "    \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 4., 5., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Behind the scenes, TorchDynamo orchestrates running each \"optimized\" callable as well as the regions for which it refuses to produce `GraphModules`. The latter regions are just run by the Python interpreter. \n",
        "\n",
        "This is an important feature of TorchDynamo. Other tracing systems, like TorchScript Tracing, are \"blind\" to many operations. Because TorchDynamo looks at Python frames, however, it's capable of observing everything the interpreter is doing. This lets it observe regions of a function that may not be representable in an FX `GraphModule`, but TorchDynamo can still record and execute these regions using the Python interpreter later."
      ],
      "metadata": {
        "id": "Ryw9Ilr_Bocl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, however, TorchDynamo refuses to handle some Python. We can see this by extending our `foo_foreign` slightly."
      ],
      "metadata": {
        "id": "HeKGeG5wCx5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "@torchdynamo.optimize(dynamo_printer)\n",
        "def foo_foreign2(a, b):\n",
        "  a = a + 2\n",
        "  b = np.add(b, 1)\n",
        "  b = torch.from_numpy(b)\n",
        "  return a + b\n",
        "\n",
        "# Suppresses some SymPy warnings not relevant to what we're doing\n",
        "result = foo_foreign2(a, b.numpy())\n",
        "\n",
        "@torchdynamo.optimize(dynamo_printer)\n",
        "def foo_foreign3(a, b):\n",
        "  a = a + 2\n",
        "  b = b + 1\n",
        "  b = torch.from_numpy(b)\n",
        "  return a + b\n",
        "\n",
        "# Suppresses some SymPy warnings not relevant to what we're doing\n",
        "result = foo_foreign3(a, b.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2dtMhYAC7NF",
        "outputId": "15f24242-f31e-4c3a-b8d8-4fac0c22e9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torchdynamo.symbolic_convert: [WARNING] Graph break: numpy from user code at   File \"<ipython-input-72-c87765088ab8>\", line 6, in foo_foreign2\n",
            "    b = np.add(b, 1)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, a : torch.Tensor):\n",
            "    add = a + 2;  a = None\n",
            "    return (add,)\n",
            "    \n",
            "torchdynamo.symbolic_convert: [WARNING] Graph break: call_function args: NumpyVariable()  from user code at   File \"<ipython-input-72-c87765088ab8>\", line 7, in <graph break in foo_foreign2>\n",
            "    b = torch.from_numpy(b)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, _stack0 : torch.Tensor, a : torch.Tensor):\n",
            "    add = a + _stack0;  a = _stack0 = None\n",
            "    return (add,)\n",
            "    \n",
            "torchdynamo.convert_frame: [ERROR] WON'T CONVERT foo_foreign3 <ipython-input-72-c87765088ab8> line 13 \n",
            "due to: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/fx/proxy.py\", line 165, in create_arg\n",
            "    raise NotImplementedError(f\"argument of type: {type(a)}\")\n",
            "NotImplementedError: argument of type: <class 'numpy.ndarray'>\n",
            "\n",
            "from user code:\n",
            "   File \"<ipython-input-72-c87765088ab8>\", line 17, in foo_foreign3\n",
            "    b = torch.from_numpy(b)\n",
            "\n",
            "Set torchdynamo.config.verbose=True for more information\n",
            "==========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While TorchDynamo will respect an explicit call to NumPy, it's unhappy with calling `__add__` on a NumPy array. This is probably just a technical glitch, and not an inherent limitation, however."
      ],
      "metadata": {
        "id": "idt1E2JvDLV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance and Caching\n",
        "\n",
        "Like any tracer, TorchDynamo is only interesting if it caches effectively. This is an inherent requirement to tracing, because tracing requires running a function at least once to observe its behavior. Since the function has already been run, there's no point in optimizing it unless we're going to call it again, and unless the tracer can quickly acquire the correct trace from its cache. If tracers never used a cache then we'd be paying a performance penalty for tracing and never realizing a benefit!\n",
        "\n",
        "We can test TorchDynamo's caching empirically by seeing when it calls our \"optimizer.\""
      ],
      "metadata": {
        "id": "-wYKwd3O8QBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torchdynamo.optimize(dynamo_printer)\n",
        "def foo_easy(a, b):\n",
        "  return a - b\n",
        "\n",
        "result = foo_easy(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL033ccEEC4E",
        "outputId": "0ec30a74-ac72-4ca9-869a-3082f0c724c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
            "    sub = a - b;  a = b = None\n",
            "    return (sub,)\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The trace for foo_easy has been cached\n",
        "result = foo_easy(a, b)"
      ],
      "metadata": {
        "id": "rg-LRt6f6X4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first call to `foo_easy` invokes `dynamo_printer`, but the second doesn't. TorchDynamo recognizes that it can reuse the callable that `dynamo_printer` previously returned to execute `foo_easy` on the same inputs. \n",
        "\n",
        "Changing the dtype of the inputs will trigger another trace, however."
      ],
      "metadata": {
        "id": "nZDJJfHMEaC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = foo_easy(a.double(), b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41JBW88sEyJn",
        "outputId": "d2c656d0-3e58-48d9-e2f1-eecae84373bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
            "    sub = a - b;  a = b = None\n",
            "    return (sub,)\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which tells us that TorchDynamo's cache is encoding properties of the input to the function, like the datatype of tensors.\n",
        "\n",
        "Now let's see how it handles Python objects."
      ],
      "metadata": {
        "id": "vyUEG3HIE15L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = foo_easy(a, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7nmNBnmFCR6",
        "outputId": "1a194e4c-c013-4d4a-c388-2c68d1d19f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "def forward(self, a : torch.Tensor):\n",
            "    sub = a - 2;  a = None\n",
            "    return (sub,)\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scalar creates a new trace, and the scalar's value appears as a constant in the trace!"
      ],
      "metadata": {
        "id": "OIFo4nnPFIj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The same scalar value doesn't cause a retrace\n",
        "result = foo_easy(a, 2)"
      ],
      "metadata": {
        "id": "HRCb-HqYE1KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... but a different scalar value does!\n",
        "result = foo_easy(a, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXykx-ZBEVUU",
        "outputId": "936e3874-31c1-4b4c-8a07-1a8fe3793777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "def forward(self, a : torch.Tensor):\n",
            "    sub = a - 3;  a = None\n",
            "    return (sub,)\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different values for the Python scalar cause TorchDynamo to retrace, suggesting that using it for functions that accept frequently changing native Python types is a bad idea. Let's compare the performance of running the same function using the Python interpreter and then using TorchDynamo as we vary the value of the scalar to see. For this test we'll use a different \"optimizer\" that doesn't print so we're not overwhelmed with printing."
      ],
      "metadata": {
        "id": "byhS3BwiFVO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torchdynamo.reset()\n",
        "\n",
        "# Simple TorchDynamo \"optimizer\" that does nothing\n",
        "def dynamo_passthrough(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
        "    return gm.forward\n",
        "\n",
        "def foo_python(a, b):\n",
        "  return a - b\n",
        "\n",
        "# Constructs the TorchDynamo traced version of the above\n",
        "foo_passthrough = torchdynamo.optimize(dynamo_passthrough)(foo_python)\n",
        "\n",
        "import time\n",
        "\n",
        "# Times TorchDynamo \n",
        "start = time.time()\n",
        "\n",
        "for b in range(1000):\n",
        "  foo_passthrough(a, b)\n",
        "\n",
        "end = time.time()\n",
        "elapsed = end - start\n",
        "print(f\"TorchDynamo elapsed time: {elapsed}\")\n",
        "\n",
        "# Times using the Python interpreter\n",
        "start = time.time()\n",
        "\n",
        "for b in range(1000):\n",
        "  foo_python(a, b)\n",
        "\n",
        "end = time.time()\n",
        "elapsed = end - start\n",
        "print(f\"Python elapsed time: {elapsed}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajfx5DI32R0F",
        "outputId": "2bbfb9ae-2dab-49ec-c84e-ca4e5db892d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torchdynamo.convert_frame: [WARNING] torchdynamo hit config.cache_size_limit (64)\n",
            "   function: 'foo_python' (<ipython-input-82-eaf361744106>:6)\n",
            "   reasons:  ['b == 0']\n",
            "to diagnose recompilation issues, see https://github.com/pytorch/torchdynamo/blob/main/TROUBLESHOOTING.md.\n",
            "TorchDynamo elapsed time: 0.21825957298278809\n",
            "Python elapsed time: 0.004097938537597656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The profiling in the above cell is very simple, but it highlights TorchDynamo's current issue handling Python objects with changing values. TorchDynamo will even warn about \"recompilation\" in this case -- which is just another name for \"retracing.\" \n",
        "\n",
        "This is not an inherent limitation of TorchDynamo's approach and I expect the TorchDynamo team to address this issue in the future. We can see what's happening more clearly by using `torchdynamo.explain` again:"
      ],
      "metadata": {
        "id": "IFdIpW5GGcwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explanation, out_guards, graphs, ops_per_graph, break_reasons = torchdynamo.explain(foo_passthrough, a, 2)"
      ],
      "metadata": {
        "id": "64x9vWKsHHNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(out_guards)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oWqshq82R4Q",
        "outputId": "890c5cff-6fee-41ae-8073-fd1b68068c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{Guard(name='b', source=<GuardSource.LOCAL: 0>, create_fn=<function GuardBuilder.CONSTANT_MATCH at 0x7fa9a7f59050>, is_volatile=False, guard_types=['EQUALS_MATCH'], code_list=['___check_type_id(b, 11105824)', 'b == 2'], obj_weakref=None, guarded_class_weakref=<weakref at 0x7fa9e8a2be90; to 'type' at 0xa97620 (int)>), Guard(name='a', source=<GuardSource.LOCAL: 0>, create_fn=<function GuardBuilder.TENSOR_MATCH at 0x7fa9a7f597a0>, is_volatile=False, guard_types=['TENSOR_MATCH'], code_list=None, obj_weakref=<weakref at 0x7fa9a5cba770; to 'Tensor' at 0x7fa9a5cc7ad0>, guarded_class_weakref=<weakref at 0x7fa9b40b2710; to 'torch._C._TensorMeta' at 0x65387a0 (Tensor)>)}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the formatting above isn't very nice, we can see that there's a \"guard\" for `b` that requires a `CONSTANT_MATCH`, suggesting what we've seen empirically that TorchDynamo is \"guarding\" reusing a previous trace on this value being the same. "
      ],
      "metadata": {
        "id": "urPYSXpgHh6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "*   Debate over Pep 523\n",
        "*   If you're going to trace, possibly the best way to do it\n",
        "*   Great observaibility\n",
        "*   Awesome that it produces FX `GraphModules` (easy to transform and execute)\n",
        "*   Addresses the top problems with TorchScript Tracing (too much metadata, \"blind\" to other operations)\n",
        "*   Representation of Python regions seems lacking, and not all Python regions are understood\n",
        "*   Caching model seems too restrictive, especially when working with native Python types like Numbers\n"
      ],
      "metadata": {
        "id": "qqPZ3OaGH5cb"
      }
    }
  ]
}